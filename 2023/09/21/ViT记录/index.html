<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha256-yIDrPSXHZdOZhAqiBP7CKzIwMQmRCJ8UeB8Jo17YC4o=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"0x6f6f.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.19.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"flat"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="NLP中seq2seq结构对self attention机制的引入 序列转序列结构常用于机器翻译，其分为编码器和解码器两部分。其中，编码器负责将输入文本转译为上下文编码向量，解码器负责对其进行解码，翻译为目标语言。  该上下文编码向量长度一般是固定的，导致模型对于变长输入文本的处理能力不一致。为了提升模型对于该序列的利用能力，学界引入了注意力机制根据输入序列的不同部分为其赋予不同的注意力权">
<meta property="og:type" content="article">
<meta property="og:title" content="ViT记录">
<meta property="og:url" content="https://0x6f6f.github.io/2023/09/21/ViT%E8%AE%B0%E5%BD%95/index.html">
<meta property="og:site_name" content="0x6f6f的小站">
<meta property="og:description" content="NLP中seq2seq结构对self attention机制的引入 序列转序列结构常用于机器翻译，其分为编码器和解码器两部分。其中，编码器负责将输入文本转译为上下文编码向量，解码器负责对其进行解码，翻译为目标语言。  该上下文编码向量长度一般是固定的，导致模型对于变长输入文本的处理能力不一致。为了提升模型对于该序列的利用能力，学界引入了注意力机制根据输入序列的不同部分为其赋予不同的注意力权">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://0x6f6f.github.io/2023/09/21/ViT%E8%AE%B0%E5%BD%95/1.png">
<meta property="og:image" content="https://0x6f6f.github.io/2023/09/21/ViT%E8%AE%B0%E5%BD%95/2.png">
<meta property="article:published_time" content="2023-09-21T06:22:45.000Z">
<meta property="article:modified_time" content="2023-10-24T06:34:04.507Z">
<meta property="article:author" content="28527">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://0x6f6f.github.io/2023/09/21/ViT%E8%AE%B0%E5%BD%95/1.png">


<link rel="canonical" href="https://0x6f6f.github.io/2023/09/21/ViT%E8%AE%B0%E5%BD%95/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://0x6f6f.github.io/2023/09/21/ViT%E8%AE%B0%E5%BD%95/","path":"2023/09/21/ViT记录/","title":"ViT记录"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>ViT记录 | 0x6f6f的小站</title>
  







<link rel="dns-prefetch" href="0x6f6f-waline-server.vercel.app">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">0x6f6f的小站</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#nlp%E4%B8%ADseq2seq%E7%BB%93%E6%9E%84%E5%AF%B9self-attention%E6%9C%BA%E5%88%B6%E7%9A%84%E5%BC%95%E5%85%A5"><span class="nav-number">1.</span> <span class="nav-text">NLP中seq2seq结构对self
attention机制的引入</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E8%A7%A3%E9%87%8A"><span class="nav-number">2.</span> <span class="nav-text">注意力机制的解释</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">28527</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/0x6f6f" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;0x6f6f" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:lvdifine@gmail.com" title="E-Mail → mailto:lvdifine@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://0x6f6f.github.io/2023/09/21/ViT%E8%AE%B0%E5%BD%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="28527">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="0x6f6f的小站">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="ViT记录 | 0x6f6f的小站">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          ViT记录
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-09-21 14:22:45" itemprop="dateCreated datePublished" datetime="2023-09-21T14:22:45+08:00">2023-09-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-24 14:34:04" itemprop="dateModified" datetime="2023-10-24T14:34:04+08:00">2023-10-24</time>
    </span>

  
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Waline: </span>
  
    <a title="waline" href="/2023/09/21/ViT%E8%AE%B0%E5%BD%95/#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/2023/09/21/ViT%E8%AE%B0%E5%BD%95/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1
id="nlp中seq2seq结构对self-attention机制的引入">NLP中seq2seq结构对self
attention机制的引入</h1>
<p>序列转序列结构常用于机器翻译，其分为编码器和解码器两部分。其中，编码器负责将输入文本转译为上下文编码向量，解码器负责对其进行解码，翻译为目标语言。</p>
<img src="/2023/09/21/ViT%E8%AE%B0%E5%BD%95/1.png" class="" title="Encoder-Decoder架构">
<p>该上下文编码向量长度一般是固定的，导致模型对于变长输入文本的处理能力不一致。为了提升模型对于该序列的利用能力，学界引入了注意力机制根据输入序列的不同部分为其赋予不同的注意力权重。</p>
<p>对注意力机制进行抽象，可以归结为Q,K,V三个矩阵的计算，如下图所示：</p>
<img src="/2023/09/21/ViT%E8%AE%B0%E5%BD%95/2.png" class="" title="QKV机制下的注意力">
<p>一个注意力层会在内部维护q,k,v三个权重矩阵，分别对应查询权重、键权重、值权重三方面权重。将输入序列向量与q,k,v矩阵分别相乘，就得到了Q（Query，查询向量，包含当前位置的输出序列信息，用于计算注意力权重）,K（Keys，键向量，包含序列中每个位置的信息，用于计算注意力权重）,V（Values，值向量，包含输入序列中每个位置的具体数值信息，用于根据注意力权重计算输出的上下文向量值）三个向量。</p>
<span id="more"></span>
<p>可以使用下面的公式表示从输入向量 x、待学习的 qkv
三个权重矩阵出发，得到注意力值的计算过程：</p>
<aside>
<p>💡 1. 首先，对输入向量 x
进行线性变换得到查询（q）、键（k）和值（v）的向量表示。其中，<span
class="math inline">\(W_q\)</span>、 <span
class="math inline">\(W_q\)</span>和 <span
class="math inline">\(W_q\)</span>分别是待学习的查询、键和值的权重矩阵：</p>
<p><span class="math display">\[
q = W_q \cdot x \\
k = W_k \cdot x \\
v = W_v \cdot x \\
\]</span></p>
<ol type="1">
<li>接下来，通过计算查询向量 q 和键向量 k
之间的相似度来计算注意力权重，其中，<span
class="math inline">\(d_k\)</span>是查询和键的向量维度：</li>
</ol>
<p><span class="math display">\[
\text{Attention}(q, k) = \text{softmax}\left(\frac{q \cdot
k^T}{\sqrt{d_k}}\right)
\]</span></p>
<ol type="1">
<li>最后，根据注意力权重对值向量 v
进行加权求和，得到注意力层输出的上下文向量：</li>
</ol>
<p><span class="math display">\[
\text{Value} = \text{Attention}(q, k) \cdot v
\]</span></p>
</aside>
<p>对上述操作的理解：</p>
<ul>
<li>qkv三个矩阵的计算可以理解为使用对应权重对输入向量的线性变换</li>
<li></li>
</ul>
<h1 id="注意力机制的解释">注意力机制的解释</h1>
<p>从上述例子中我们可以知道，注意力机制的引入目的是“有的放矢”地处理中间向量，提高模型效率。其实在自注意力机制概念提出之前，已经有其他意义比较相近的研究被提出：</p>
<ol type="1">
<li><p>自下而上的基于显著性的注意力机制：</p>
<ul>
<li>maxpooling 最大池化</li>
<li>geating 门控</li>
</ul></li>
<li><p>自上而下的汇聚式注意力</p>
<p>概念：从更高级别的表示或上下文信息开始，逐渐聚焦于更低级别的表示或局部信息</p>
<aside>
<p>💡 procedure:</p>
<p>首先通过某种方式生成一个<strong>全局的上下文向量</strong>，通常是通过对整个输入序列做一次汇总或编码得到的。这个全局上下文向量可以被视为任务相关的高级表示，它包含了任务需要关注的重要信息。</p>
<p>然后，针对输入序列中的每个元素，<strong>计算其与全局上下文向量之间的注意力权重</strong>。这些权重表示了每个元素对于全局上下文的重要性，即它们在给定任务中的相对重要程度。</p>
<p>最后，根据计算得到的注意力权重，将各个元素的表示进行加权求和，得到最终的上下文表示。这个上下文表示综合了全局上下文和局部信息，可以用于后续的任务处理或决策。</p>
</aside>
<p>对于一个待加入注意力的输入序列<span
class="math inline">\(X=(x_1,x_2,...,x_N)\)</span>，其中N表示信息项数目，设<span
class="math inline">\(z\in\{1,2,...,N\}\)</span>表示被选择信息索引位置，给定一个和任务相关的查询（Query）向量q，则z=i的信息项的注意力值可以表示为：</p>
<p><span class="math display">\[
p(z=i|X,q)=softmax(s(x_i,q))
\]</span></p>
<p>其中<span
class="math inline">\(s(x_i,q)\)</span>为注意力打分函数，一般采用缩放点积形式来定义。注意这里面的d表示输入信息的维度。</p>
<p><span class="math display">\[
s(x_i,q)=\frac{x_i^Tq}{\sqrt{d}}
\]</span></p>
<aside>
<p>💡 用缩放点积形式定义注意力打分函数的原因</p>
<ol type="1">
<li>数值稳定性：缩放点积可以通过对点积结果进行缩放，将注意力分数限制在一个合适的范围内，避免了数值过大或过小的问题。通过缩放，可以使得注意力分数在相对较小的范围内，更利于数值计算的稳定性。</li>
<li>温度调节：缩放点积中的缩放因子通常称为温度参数（temperature
parameter），可以调节注意力的分布形状。较高的温度会使得注意力更加平滑均匀，较低的温度会使得注意力更加集中和尖锐。温度参数的调节可以根据具体任务和需求来进行优化。
</aside></li>
</ol></li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
            </div>
            <div class="post-nav-item">
                <a href="/2023/09/21/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E5%92%8C%E5%86%85%E5%AE%B9%E5%8F%91%E5%B8%83%E8%A7%84%E5%88%92/" rel="next" title="博客搭建和内容发布规划">
                  博客搭建和内容发布规划 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="waline"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">28527</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="waline" type="application/json">{"lang":"zh-cn","enable":true,"serverURL":"0x6f6f-waline-server.vercel.app","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":true,"pageview":false,"placeholder":"请文明评论呀","avatar":"mm","meta":["nick","mail","link"],"pageSize":10,"visitor":true,"comment_count":true,"requiredFields":[],"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","el":"#waline","comment":true,"path":"/2023/09/21/ViT%E8%AE%B0%E5%BD%95/"}</script>
<link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>

</body>
</html>

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.0">

  <link rel="apple-touch-icon" sizes="180x180" href="C:\Users\Aressions\blog\themes\next\source\images\faviconv1\apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="C:\Users\Aressions\blog\themes\next\source\images\faviconv1\favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="C:\Users\Aressions\blog\themes\next\source\images\faviconv1\favicon-16x16.png">
  <link rel="mask-icon" href="C:\Users\Aressions\blog\themes\next\source\images\faviconv1\favicon-16x16.png" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha256-yIDrPSXHZdOZhAqiBP7CKzIwMQmRCJ8UeB8Jo17YC4o=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"0x6f6f.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.19.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"default"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Background 目的：从背景信息中挑选对当前任务目标更关键的信息。 应用场景：序列数据处理 机制分类：自注意力机制、空间注意力机制、时间注意力机制。 Classificiation   点积自注意力机制 NLP中自注意力机制的计算步骤：  预处理输入数据X 初始化权重\(W_Q,W_K,W_V\) 计算K,Q,V矩阵（仅限于输入部分的编码过程，encoder输出到">
<meta property="og:type" content="article">
<meta property="og:title" content="自注意力机制">
<meta property="og:url" content="https://0x6f6f.github.io/2023/10/27/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/index.html">
<meta property="og:site_name" content="0x6f6f的小站">
<meta property="og:description" content="Background 目的：从背景信息中挑选对当前任务目标更关键的信息。 应用场景：序列数据处理 机制分类：自注意力机制、空间注意力机制、时间注意力机制。 Classificiation   点积自注意力机制 NLP中自注意力机制的计算步骤：  预处理输入数据X 初始化权重\(W_Q,W_K,W_V\) 计算K,Q,V矩阵（仅限于输入部分的编码过程，encoder输出到">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://0x6f6f.github.io/2023/10/27/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/Untitled.png">
<meta property="og:image" content="https://0x6f6f.github.io/2023/10/27/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/Untitled_1.png">
<meta property="og:image" content="https://0x6f6f.github.io/2023/10/27/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/Untitled_2.png">
<meta property="og:image" content="https://0x6f6f.github.io/2023/10/27/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84.png">
<meta property="og:image" content="https://0x6f6f.github.io/2023/10/27/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/%E7%BF%BB%E8%AF%91%E5%99%A8.png">
<meta property="article:published_time" content="2023-10-27T09:33:33.000Z">
<meta property="article:modified_time" content="2023-10-29T14:03:16.865Z">
<meta property="article:author" content="0x6f6f">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://0x6f6f.github.io/2023/10/27/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/Untitled.png">


<link rel="canonical" href="https://0x6f6f.github.io/2023/10/27/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://0x6f6f.github.io/2023/10/27/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/","path":"2023/10/27/自注意力机制/","title":"自注意力机制"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>自注意力机制 | 0x6f6f的小站</title>
  







<link rel="dns-prefetch" href="0x6f6f-waline-server.vercel.app">
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">0x6f6f的小站</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>







</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="0x6f6f"
      src="/images/faviconv1/favicon-32x32.png">
  <p class="site-author-name" itemprop="name">0x6f6f</p>
  <div class="site-description" itemprop="description">不积跬步，无以至千里。</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">8</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/0x6f6f" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;0x6f6f" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:lvdifine@gmail.com" title="E-Mail → mailto:lvdifine@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://0x6f6f.github.io/2023/10/27/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/faviconv1/favicon-32x32.png">
      <meta itemprop="name" content="0x6f6f">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="0x6f6f的小站">
      <meta itemprop="description" content="不积跬步，无以至千里。">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="自注意力机制 | 0x6f6f的小站">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          自注意力机制
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2023-10-27 17:33:33" itemprop="dateCreated datePublished" datetime="2023-10-27T17:33:33+08:00">2023-10-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2023-10-29 22:03:16" itemprop="dateModified" datetime="2023-10-29T22:03:16+08:00">2023-10-29</time>
    </span>

  
    <span id="/2023/10/27/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" class="post-meta-item leancloud_visitors" data-flag-title="自注意力机制" title="Views">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span class="leancloud-visitors-count"></span>
    </span>
  
  <span class="post-meta-item">
    
    <span class="post-meta-item-icon">
      <i class="far fa-comment"></i>
    </span>
    <span class="post-meta-item-text">Waline: </span>
  
    <a title="waline" href="/2023/10/27/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/#waline" itemprop="discussionUrl">
      <span class="post-comments-count waline-comment-count" data-path="/2023/10/27/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/" itemprop="commentCount"></span>
    </a>
  </span>
  
  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><ol type="1">
<li><p>Background</p>
<p>目的：从背景信息中挑选对当前任务目标更关键的信息。</p>
<p>应用场景：序列数据处理</p>
<p>机制分类：自注意力机制、空间注意力机制、时间注意力机制。</p></li>
<li><p>Classificiation</p></li>
</ol>
<ul>
<li><p>点积自注意力机制</p>
<p>NLP中自注意力机制的计算步骤：</p>
<ul>
<li><p>预处理输入数据X</p></li>
<li><p>初始化权重<span
class="math inline">\(W_Q,W_K,W_V\)</span></p></li>
<li><p>计算K,Q,V矩阵（仅限于输入部分的编码过程，encoder输出到decoder时的QKV不通过该方式计算）</p>
<p><span class="math display">\[
  \begin{cases}K=XW_K\\Q=XW_Q\\V=XW_V\end{cases}
  \]</span></p></li>
<li><p>计算注意力得分：<span
class="math inline">\(softmax(\frac{QK^T}{\sqrt{d_k}})\)</span>，然后再和V相乘</p></li>
<li><p>得到自注意力矩阵<span
class="math inline">\(softmax(\frac{QK^T}{\sqrt{d_k}})V\)</span></p></li>
</ul>
<p>其中，输入矩阵X通过乘以对应权重会生成对应的QKV矩阵，分别表征：</p>
<ul>
<li>Q：查询向量，代表需要关注的元素或者位置。</li>
<li>K：键向量，代表参考元素或者位置。作用是用于提供参考信息，来确定序列中某一位置的元素相比于其他位置的元素的相关性。</li>
<li>V：值向量，表示实际信息。输入向量X所在的空间不一定适用所有类型的任务的需要，比如一个数据在低秩空间可能是线性不可分的，但将其映射到高维空间后就可以找到一个明显的分界点。因此通过乘以一个可学习的权重矩阵<span
class="math inline">\(W_V\)</span>，我们可以自适应的调节模型对输入向量的映射，从而提升模型对输入信息的学习能力。</li>
</ul>
<p>然后，我们关注一下注意力得分的计算公式：</p>
<ul>
<li><p>首先是Q和<span
class="math inline">\(K^T\)</span>的内积，这一项的意义在于度量Q和K两个向量的相似度。</p>
<p>考虑一组简单的二维向量： <span id="more"></span> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#固定的vector_a</span></span><br><span class="line">vector_a = np.array([<span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#夹角变化的vector_b，模长也是1，在单位圆上滑动</span></span><br><span class="line">angle_degrees = [<span class="number">15</span>, <span class="number">45</span>, <span class="number">75</span>, <span class="number">105</span>]</span><br><span class="line">vector_b = np.array([np.cos(np.radians(angle)), np.sin(np.radians(angle))])</span><br></pre></td></tr></table></figure></p>
<p>我们编写如下脚本做一下可视化：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个模长固定的向量A</span></span><br><span class="line">vector_a = np.array([<span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个包含不同夹角的列表</span></span><br><span class="line">angle_degrees = [<span class="number">15</span>, <span class="number">45</span>, <span class="number">75</span>, <span class="number">105</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个子图</span></span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="built_in">len</span>(angle_degrees), figsize=(<span class="number">16</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个列表来存储内积值</span></span><br><span class="line">dot_products = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, angle <span class="keyword">in</span> <span class="built_in">enumerate</span>(angle_degrees):</span><br><span class="line">    <span class="comment"># 计算向量B的坐标</span></span><br><span class="line">    vector_b = np.array([np.cos(np.radians(angle)), np.sin(np.radians(angle))])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算内积</span></span><br><span class="line">    dot_product = np.dot(vector_a, vector_b)</span><br><span class="line">    dot_products.append(dot_product)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建Seaborn风格的图形</span></span><br><span class="line">    sns.<span class="built_in">set</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 绘制向量</span></span><br><span class="line">    axes[i].quiver(<span class="number">0</span>, <span class="number">0</span>, vector_a[<span class="number">0</span>], vector_a[<span class="number">1</span>], angles=<span class="string">&#x27;xy&#x27;</span>, scale_units=<span class="string">&#x27;xy&#x27;</span>, scale=<span class="number">1</span>, color=<span class="string">&#x27;r&#x27;</span>, label=<span class="string">&#x27;Vector A&#x27;</span>)</span><br><span class="line">    axes[i].quiver(<span class="number">0</span>, <span class="number">0</span>, vector_b[<span class="number">0</span>], vector_b[<span class="number">1</span>], angles=<span class="string">&#x27;xy&#x27;</span>, scale_units=<span class="string">&#x27;xy&#x27;</span>, scale=<span class="number">1</span>, color=<span class="string">&#x27;b&#x27;</span>, label=<span class="string">f&#x27;Vector B (<span class="subst">&#123;angle&#125;</span> degrees)&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置坐标轴范围</span></span><br><span class="line">    axes[i].set_xlim(-<span class="number">1.2</span>, <span class="number">1.2</span>)</span><br><span class="line">    axes[i].set_ylim(-<span class="number">1.2</span>, <span class="number">1.2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 添加标签</span></span><br><span class="line">    axes[i].set_xlabel(<span class="string">&#x27;X-axis&#x27;</span>)</span><br><span class="line">    axes[i].set_ylabel(<span class="string">&#x27;Y-axis&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 显示夹角和内积值</span></span><br><span class="line">    axes[i].text(-<span class="number">0.5</span>, -<span class="number">0.2</span>, <span class="string">f&#x27;Angle: <span class="subst">&#123;angle&#125;</span> degrees&#x27;</span>, ha=<span class="string">&#x27;left&#x27;</span>)</span><br><span class="line">    axes[i].text(-<span class="number">0.5</span>, -<span class="number">0.4</span>, <span class="string">f&#x27;Dot Product: <span class="subst">&#123;dot_product:<span class="number">.2</span>f&#125;</span>&#x27;</span>, ha=<span class="string">&#x27;left&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 添加网格</span></span><br><span class="line">    axes[i].grid(<span class="literal">True</span>)</span><br><span class="line">    axes[i].legend(loc=<span class="string">&#x27;lower left&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置子图之间的间隔</span></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.savefig(<span class="string">&#x27;/data/mmdSTTL/文档/compare.jpg&#x27;</span>)</span><br></pre></td></tr></table></figure></p>
<p>有如下的对比：</p>
<p><img src="/2023/10/27/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/Untitled.png" class="" title="点积值与张量夹角"></p>
<p>可见随着夹角从锐角向钝角变化，两个向量的内积值越来越小。因此我们可以认为两个向量内积值越大，它们之间的相似程度越高。因此可以用向量之间的内积值度量其相似性。</p>
<p>另外，由于Q和K都是来自于输入X的变换，而向量X可能不一定是方阵，所以必须得让K转置一下才能让两个向量求内积。所以体现到注意力公式里面就是<span
class="math inline">\(QK^T\)</span>。</p></li>
<li><p>然后是后续处理过程。我们通过除以对内积值做了一次重整，即<span
class="math inline">\(\frac{QK^T}{\sqrt{d_k}}\)</span>。这样做的意义在于缩放方差，使得方差较大的内积矩阵在softmax后尽量平滑。我们也可以通过如下脚本做一下可视化：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个复杂的输入矩阵</span></span><br><span class="line">x = np.array([[<span class="number">1</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">8</span>],</span><br><span class="line">              [<span class="number">2</span>, <span class="number">7</span>, <span class="number">4</span>, <span class="number">9</span>],</span><br><span class="line">              [<span class="number">6</span>, <span class="number">2</span>, <span class="number">7</span>, <span class="number">3</span>],</span><br><span class="line">              [<span class="number">5</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义未除以sqrt(d_k)的softmax函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">x</span>):</span><br><span class="line">    e_x = np.exp(x - np.<span class="built_in">max</span>(x))</span><br><span class="line">    <span class="keyword">return</span> e_x / e_x.<span class="built_in">sum</span>(axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义除以sqrt(d_k)的softmax函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">scaled_softmax</span>(<span class="params">x, sqrt_dk</span>):</span><br><span class="line">    e_x = np.exp(x * sqrt_dk - np.<span class="built_in">max</span>(x))</span><br><span class="line">    <span class="keyword">return</span> e_x / e_x.<span class="built_in">sum</span>(axis=<span class="number">0</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设 d_k = 4</span></span><br><span class="line">sqrt_dk = <span class="number">1.0</span> / np.sqrt(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算未除以sqrt(d_k)和除以sqrt(d_k)后的softmax输出</span></span><br><span class="line">softmax_output = softmax(x)</span><br><span class="line">scaled_softmax_output = scaled_softmax(x, sqrt_dk)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建热图来比较两者</span></span><br><span class="line">fig, axes = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">12</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制未除以sqrt(d_k)的softmax输出</span></span><br><span class="line">cax1 = axes[<span class="number">0</span>].matshow(softmax_output, cmap=<span class="string">&#x27;viridis&#x27;</span>, aspect=<span class="string">&#x27;auto&#x27;</span>)</span><br><span class="line">plt.colorbar(cax1, ax=axes[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在每个格子中标注数字</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(x.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(x.shape[<span class="number">1</span>]):</span><br><span class="line">        axes[<span class="number">0</span>].text(j, i, <span class="string">f&#x27;<span class="subst">&#123;softmax_output[i, j]:<span class="number">.2</span>f&#125;</span>&#x27;</span>, ha=<span class="string">&#x27;center&#x27;</span>, va=<span class="string">&#x27;center&#x27;</span>, color=<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line"></span><br><span class="line">axes[<span class="number">0</span>].set_title(<span class="string">&#x27;Softmax (Unscaled)&#x27;</span>)</span><br><span class="line">axes[<span class="number">0</span>].xaxis.set_ticks_position(<span class="string">&#x27;top&#x27;</span>)</span><br><span class="line">axes[<span class="number">0</span>].xaxis.set_label_position(<span class="string">&#x27;top&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制除以sqrt(d_k)后的softmax输出</span></span><br><span class="line">cax2 = axes[<span class="number">1</span>].matshow(scaled_softmax_output, cmap=<span class="string">&#x27;viridis&#x27;</span>, aspect=<span class="string">&#x27;auto&#x27;</span>)</span><br><span class="line">plt.colorbar(cax2, ax=axes[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在每个格子中标注数字</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(x.shape[<span class="number">0</span>]):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(x.shape[<span class="number">1</span>]):</span><br><span class="line">        axes[<span class="number">1</span>].text(j, i, <span class="string">f&#x27;<span class="subst">&#123;scaled_softmax_output[i, j]:<span class="number">.2</span>f&#125;</span>&#x27;</span>, ha=<span class="string">&#x27;center&#x27;</span>, va=<span class="string">&#x27;center&#x27;</span>, color=<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line"></span><br><span class="line">axes[<span class="number">1</span>].set_title(<span class="string">&#x27;Scaled Softmax&#x27;</span>)</span><br><span class="line">axes[<span class="number">1</span>].xaxis.set_ticks_position(<span class="string">&#x27;top&#x27;</span>)</span><br><span class="line">axes[<span class="number">1</span>].xaxis.set_label_position(<span class="string">&#x27;top&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.savefig(<span class="string">&#x27;/data/mmdSTTL/文档/compare.jpg&#x27;</span>)</span><br></pre></td></tr></table></figure></p>
<p>上述脚本的输出结果如下：</p>
<p><img src="/2023/10/27/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/Untitled_1.png" class="" title="scale的效果"></p>
<blockquote>
<p>Terminal output:</p>
<p>Regular Softmax Output Variance: 0.004457626216288582 Scaled Softmax
Output Variance: 0.0011144065540721454</p>
</blockquote>
<p>可以看到，在scale（该例子中<span
class="math inline">\(d_k=4\)</span>，即维度为4*4）后，softmax的输出矩阵方差变得更小，输出结果更加平滑。</p>
<p>至于softmax本身，老生常谈了，他的作用就是使得分布拉伸到总和为1的区间，使得输出值具有概率分布特征，可以用于度量相关性。对矩阵的softmax操作可以简单的表示为：</p>
<p><span class="math display">\[
  \text{Softmax}(X)_{ij} = \frac{e^{x_{ij}}}{\sum_{k} e^{x_{ik}}}
  \]</span></p>
<p>我们对一个输入的一位序列做一下softmax并校验其性质：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一个随机的一维序列</span></span><br><span class="line">sequence = np.random.rand(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 softmax</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">x</span>):</span><br><span class="line">    e_x = np.exp(x - np.<span class="built_in">max</span>(x))</span><br><span class="line">    <span class="keyword">return</span> e_x / e_x.<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 softmax 进行平滑</span></span><br><span class="line">smoothed_sequence = softmax(sequence)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据框用于Seaborn绘图</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.DataFrame(&#123;<span class="string">&#x27;Index&#x27;</span>: <span class="built_in">range</span>(<span class="built_in">len</span>(sequence)), <span class="string">&#x27;Original Value&#x27;</span>: sequence, <span class="string">&#x27;Smoothed Value&#x27;</span>: smoothed_sequence&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Seaborn绘制散点图，并连接同一个序列中的点</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">6</span>))</span><br><span class="line">plt.title(<span class="string">&quot;Connected Scatter Plot of Original and Smoothed Sequence&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Index&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Value&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制原始序列的点并添加标签</span></span><br><span class="line">sns.lineplot(data=data, x=<span class="string">&#x27;Index&#x27;</span>, y=<span class="string">&#x27;Original Value&#x27;</span>, label=<span class="string">f&quot;Original Sequence: sum=<span class="subst">&#123;sequence.<span class="built_in">sum</span>()&#125;</span>&quot;</span>, color=<span class="string">&#x27;b&#x27;</span>, marker=<span class="string">&quot;o&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制平滑后的序列的点并添加标签</span></span><br><span class="line">sns.lineplot(data=data, x=<span class="string">&#x27;Index&#x27;</span>, y=<span class="string">&#x27;Smoothed Value&#x27;</span>, label=<span class="string">f&quot;Smoothed Sequence: sum=<span class="subst">&#123;smoothed_sequence.<span class="built_in">sum</span>()&#125;</span>&quot;</span>, color=<span class="string">&#x27;r&#x27;</span>, marker=<span class="string">&quot;o&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加图例</span></span><br><span class="line">plt.legend()</span><br><span class="line">plt.savefig(<span class="string">&#x27;/data/mmdSTTL/文档/softmax.jpg&#x27;</span>)</span><br></pre></td></tr></table></figure></p>
<p><img src="/2023/10/27/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/Untitled_2.png" class="" title="softmax的效果"></p>
<p>比如我们看上面这张图里面，softmax
函数将输入的元素转化为一个概率分布，使得序列的每个元素都在 0 到 1
的范围内，并且所有元素的和等于 1。</p></li>
</ul>
<p>最后我们将结果乘上值矩阵V，就等于我们拿着算出来的权重给被查询的转换后的分布赋了一次权。因为<span
class="math inline">\(QK^T\)</span>是<span
class="math inline">\((m,n)\times(n,m)=(m,m)\)</span>，所以可以直接乘上规模为<span
class="math inline">\((m,n)\)</span>的矩阵<span
class="math inline">\(V\)</span>。</p></li>
<li><p>加性注意力机制</p></li>
<li><p>带参数注意力计算</p></li>
</ul>
<ol type="1">
<li><p>How Attention Mechanisms works in Decoder and Encoder</p>
<p>记注意力层的输入头分别为q,k,v，则：</p>
<p><img src="/2023/10/27/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84.png" class="" title="Transformer模型Encoder, Decoder的细节图（省去了FFN部分）"></p>
<ul>
<li><p>Encoder：只涉及自注意力机制，q,k,v都来自于上一层输出。即Encoder只会计算序列内部的相关性。</p></li>
<li><p>Decoder：涉及交叉注意力，q是来自于上一层输出的，但是kv两者都来自于一个encoder的输出。</p>
<blockquote>
<p>decoder中的cross-attention的query对应了目标端序列，key,
value对应了源端序列;</p>
</blockquote>
<p>注意，第一个Decoder的输入是右移后（因为<strong>开始的时候，decoder的输入是一个特殊的起始符</strong>）的本模型上一次的预测结果；另外，decoder中的自注意力机制是masked的。这里分别进行解释：</p>
<ul>
<li><p>mask：用于掩盖未来信息，避免模型在训练时过度依赖这些不应该被用来生成信息的数据。</p>
<blockquote>
<p>We also modify the self-attention sub-layer in the decoder stack to
prevent from attending to subsequent positions. This masking, combined
with the fact that the output embeddings are offset by one position,
ensures that the predictions for position i can depend only on the known
outputs at positions less than i.</p>
</blockquote>
<p>解释：在推理timestep=T的token时，decoder只能“看到”timestep &lt; T的
T-1 个Token,
不能和timestep大于它自身的token做attention（因为根本还不知道后面的token是什么）。为了保证训练时和推理时的一致性，所以，训练时要同样防止token与它之后的token去做attention。</p>
<p>实现：在Decoder做self
attention时，初始化一个下三角矩阵为0，上三角元素均为负无穷的矩阵加到注意力矩阵（这里指的是自注意机制中，点积或者加性注意力算出来的那个注意力矩阵）上。</p></li>
<li><p>第一个Decoder的输入是右移后的本模型上一次的预测结果</p>
<p>输入来源：</p>
<ol type="1">
<li><strong>Special
Token</strong>：通常，解码器在处理第一个时间步的输入时，会提供一个特殊的开始标记（如<strong><code>&lt;/S&gt;</code></strong>，表示"开始"），作为初始的输入。这个特殊标记告诉解码器开始生成目标端序列。</li>
<li><strong>目标端序列</strong>：此外，解码器还接收整个目标端序列作为输入，尽管它在初始时不会使用整个序列。这是为了帮助模型学习如何在生成过程中依赖于目标序列的上下文信息。在初始时间步，解码器只会使用"Special
Token"和自注意力机制来生成第一个目标词。</li>
</ol>
<p>右移：开始的时候，decoder的输入是一个起始符。因此在循环地给decoder喂数据的时候，输入序列是右移一位了的输入序列。所以第一个Decoder的输入是右移后的本模型上一次的预测结果。</p>
<p>具体流程可以参照以下图片：</p>
<p><img src="/2023/10/27/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/%E7%BF%BB%E8%AF%91%E5%99%A8.png" class="" title="一个transformer翻译器的例子"></p>
<p>一个transformer翻译器的例子</p></li>
</ul></li>
</ul></li>
<li><p>Multi-head Attention: heading higher data dimensions</p>
<blockquote>
<p>多头注意力机制能够在<strong>不改变参数量的情况下增强每一层attention对输入序列的表示能力</strong>。</p>
</blockquote>
<p>参考以下代码：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, n_head</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__()</span><br><span class="line">        self.n_head = n_head</span><br><span class="line">        self.attention = ScaleDotProductAttention()</span><br><span class="line">        self.w_q = nn.Linear(d_model, d_model)</span><br><span class="line">        self.w_k = nn.Linear(d_model, d_model)</span><br><span class="line">        self.w_v = nn.Linear(d_model, d_model)</span><br><span class="line">        self.w_concat = nn.Linear(d_model, d_model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, q, k, v, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># 1. dot product with weight matrices</span></span><br><span class="line">        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. split tensor by number of heads</span></span><br><span class="line">        q, k, v = self.split(q), self.split(k), self.split(v)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3. do scale dot product to compute similarity</span></span><br><span class="line">        out, attention = self.attention(q, k, v, mask=mask)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 4. concat and pass to linear layer</span></span><br><span class="line">        out = self.concat(out)</span><br><span class="line">        out = self.w_concat(out)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 5. visualize attention map</span></span><br><span class="line">        <span class="comment"># TODO : we should implement visualization</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">split</span>(<span class="params">self, tensor</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        split tensor by number of head</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param tensor: [batch_size, length, d_model]</span></span><br><span class="line"><span class="string">        :return: [batch_size, head, length, d_tensor]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        batch_size, length, d_model = tensor.size()</span><br><span class="line"></span><br><span class="line">        d_tensor = d_model // self.n_head</span><br><span class="line">        tensor = tensor.view(batch_size, length, self.n_head, d_tensor).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">				</span><br><span class="line">        <span class="comment"># it is similar with group convolution (split by number of heads)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> tensor</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">concat</span>(<span class="params">self, tensor</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        inverse function of self.split(tensor : torch.Tensor)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param tensor: [batch_size, head, length, d_tensor]</span></span><br><span class="line"><span class="string">        :return: [batch_size, length, d_model]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        batch_size, head, length, d_tensor = tensor.size()</span><br><span class="line">        d_model = head * d_tensor</span><br><span class="line"></span><br><span class="line">        tensor = tensor.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, length, d_model)</span><br><span class="line">        <span class="keyword">return</span> tensor</span><br></pre></td></tr></table></figure></p>
<p>多头注意力的计算过程大概可以理解为将输入序列拆分成多个子序列，再对每个子序列分别计算注意力矩阵，然后再拼接结果。</p>
<ul>
<li>MHA接受两个参数：
<ul>
<li>d_model：隐藏状态维度/嵌入维度。这一般指的是一个 token
通过嵌入层（embedding layer）计算得出的高维表示。</li>
<li>n_head：注意力头数。这指的是需要将输入序列拆分成几个子序列；我们随后会调用split方法对qkv三个向量都进行拆分再进行注意力计算。</li>
</ul></li>
<li>MHA接受的向量输入规格为<strong>[batch_size, seq_length,
d_model]</strong>，其意义为：
<ul>
<li>batch_size：批次内数据量</li>
<li>seq_length：输入序列的长度，即序列内token数</li>
<li>d_model：嵌入维度，即一个token映射得到的高维表示</li>
</ul></li>
</ul>
<p>对于MHA内部实现的拆分和合并机制，可以做以下理解：我们把一个高维特征平均拆分成n_head份，然后对拆分后的各个qkv分别计算注意力矩阵，再分别乘上各自的值矩阵，最后再合并成完整的特征表示。</p>
<ul>
<li><p>拆分部分的逻辑：直接将计算后得到的Q,K,V矩阵拆掉就行，后面直接使用这些子序列迭代进行计算。</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">split</span>(<span class="params">self, tensor</span>):</span><br><span class="line">       <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">       split tensor by number of head</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">       :param tensor: [batch_size, length, d_model]</span></span><br><span class="line"><span class="string">       :return: [batch_size, head, length, d_tensor]</span></span><br><span class="line"><span class="string">       &quot;&quot;&quot;</span></span><br><span class="line">       batch_size, length, d_model = tensor.size()</span><br><span class="line">        </span><br><span class="line">       d_tensor = d_model // self.n_head</span><br><span class="line">       tensor = tensor.view(batch_size, length, self.n_head, d_tensor).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">       <span class="comment"># it is similar with group convolution (split by number of heads)</span></span><br><span class="line">        </span><br><span class="line">       <span class="keyword">return</span> tensor</span><br></pre></td></tr></table></figure></p>
<p>注意，在tensor那一行执行了两次形状调整：首先将tensor通过view方式改成（B,L,N,D），然后是将L,N交换，最后的输出维度是（batch_size,
self.n_head, length,
d_tensor），这样方便我们对拆出来的每个子矩阵做转置（<span
class="math inline">\(Q_iK_i^T\)</span>会用到）；同时也符合ScaleDotProductAttention的输入格式。</p></li>
<li><p>缩放点积注意力计算：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ScaleDotProductAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    compute scale dot product attention</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Query : given sentence that we focused on (decoder)</span></span><br><span class="line"><span class="string">    Key : every sentence to check relationship with Qeury(encoder)</span></span><br><span class="line"><span class="string">    Value : every sentence same with Key (encoder)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(ScaleDotProductAttention, self).__init__()</span><br><span class="line">        self.softmax = nn.Softmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, q, k, v, mask=<span class="literal">None</span>, e=<span class="number">1e-12</span></span>):</span><br><span class="line">        <span class="comment"># input is 4 dimension tensor</span></span><br><span class="line">        <span class="comment"># [batch_size, head, length, d_tensor]</span></span><br><span class="line">        batch_size, head, length, d_tensor = k.size()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1. dot product Query with Key^T to compute similarity</span></span><br><span class="line">        k_t = k.transpose(<span class="number">2</span>, <span class="number">3</span>)  <span class="comment"># transpose</span></span><br><span class="line">        score = (q @ k_t) / math.sqrt(d_tensor)  <span class="comment"># scaled dot product</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. apply masking (opt)</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            score = score.masked_fill(mask == <span class="number">0</span>, -<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3. pass them softmax to make [0, 1] range</span></span><br><span class="line">        score = self.softmax(score)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4. multiply with Value</span></span><br><span class="line">        v = score @ v</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> v, score</span><br></pre></td></tr></table></figure></p>
<p>注意<span
class="math inline">\(K_i^T\)</span>是针对的拆出来的每个子序列，所以这里接受的输入是[B,H,L,D_T]，然后transpose转的维度也是序号为2,3的两个维度。</p>
<p>注意这里的masking。输入的mask应该是一个只包含0和1，而且形状和该模块的形参中的qkv相同的张量。score是第一步点积计算出来的张量，原生支持.masked_fill方法。我们这里使用的是score.masked_fill(mask
== 0,
-10000)，调用后会返回一个将score张量中满足mask==0的条件的位置替换成-10000的张量（-inf）。</p></li>
<li><p>合并部分的逻辑：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">concat</span>(<span class="params">self, tensor</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        inverse function of self.split(tensor : torch.Tensor)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param tensor: [batch_size, head, length, d_tensor]</span></span><br><span class="line"><span class="string">        :return: [batch_size, length, d_model]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        batch_size, head, length, d_tensor = tensor.size()</span><br><span class="line">        d_model = head * d_tensor</span><br><span class="line"></span><br><span class="line">        tensor = tensor.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, length, d_model)</span><br><span class="line">        <span class="keyword">return</span> tensor</span><br></pre></td></tr></table></figure></p>
<p>合并的目的在于将以前分开的几个注意力头合并起来，得到新的特征矩阵。该部分涉及三个操作：</p>
<ul>
<li>1,2维度转置，目的在于将张量形状调整回batch_size, length, head,
d_tensor，方便后面合并。</li>
<li>调用.contiguous()方法将转置后的张量转变为连续存储。因为转置、veiw、切片等操作都可能会破坏张量在显存中存储的连续性，因此有必要在操作后调用contiguous方法保证其连续性。注意pytorch现在在大部分情况都会自动尝试保证张量连续性。</li>
<li>最后是view，把后两个维度合并到一起。这在逻辑上类似于把三段d_tensor首尾相连拼到一起。</li>
</ul></li>
</ul></li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/09/21/%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9B%B8%E5%85%B3/" rel="prev" title="服务器相关">
                  <i class="fa fa-angle-left"></i> 服务器相关
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/12/31/DETR-survey/" rel="next" title="DETR-survey">
                  DETR-survey <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="waline"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2023 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">0x6f6f@cqu</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div><script color="0,0,0" opacity="0.5" zIndex="-1" count="99" src="https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js"></script>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  




  <script src="/js/third-party/pace.js"></script>


  


  <script class="next-config" data-name="leancloud_visitors" type="application/json">{"enable":true,"app_id":"Wa3k8ylRA7dli6SMCMNYLm4A-MdYXbMMI","app_key":"qc5L2urgh6ZYQhzbS18Flzy6","server_url":null,"security":false}</script>
  <script src="/js/third-party/statistics/lean-analytics.js"></script>


  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="waline" type="application/json">{"lang":"zh-cn","enable":true,"serverURL":"0x6f6f-waline-server.vercel.app","cssUrl":"https://unpkg.com/@waline/client@v2/dist/waline.css","commentCount":true,"pageview":false,"placeholder":"我说几句……","avatar":"mm","meta":["nick","mail","link"],"pageSize":10,"visitor":false,"comment_count":true,"requiredFields":[],"libUrl":"//unpkg.com/@waline/client@v2/dist/waline.js","el":"#waline","comment":true,"path":"/2023/10/27/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"}</script>
<link rel="stylesheet" href="https://unpkg.com/@waline/client@v2/dist/waline.css">
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.waline.el).then(() =>
    NexT.utils.getScript(CONFIG.waline.libUrl, { condition: window.Waline })
  ).then(() => 
    Waline.init(Object.assign({}, CONFIG.waline,{ el: document.querySelector(CONFIG.waline.el) }))
  );
});
</script>

<!-- hexo injector body_end start --><script src="/js/outdate.js"></script><!-- hexo injector body_end end --></body>
</html>
